name: ${oc.env:PROJECT_NAME}
resources:
  - id: notebooks-dir
    service: workspace
    properties:
      source_path: ${oc.env:PROJECT_DIR}/notebooks
      path: ${oc.env:DATABRICKS_WORKSPACE_DIR}/notebooks
      object_type: DIRECTORY
  - id: src-dir
    service: dbfs
    properties:
      source_path: ${oc.env:PROJECT_DIR}/databricks_stack_example
      path: ${oc.env:DATABRICKS_DBFS_DIR}/databricks_stack_example
      is_dir: true
  - id: dist-dir
    service: dbfs
    properties:
      source_path: ${oc.env:PROJECT_DIR}/dist
      path: ${oc.env:DATABRICKS_DBFS_DIR}/dist
      is_dir: true
  - id: data-dir
    service: dbfs
    properties:
      source_path: ${oc.env:PROJECT_DIR}/data
      path: ${oc.env:DATABRICKS_DBFS_DIR}/data
      is_dir: true
  - id: single-task-job
    service: jobs
    properties:
      name: Example Single Task Job
      new_cluster:
        spark_version: 10.4.x-cpu-ml-scala2.12
        node_type_id: Standard_DS3_v2
        num_workers: 0
        spark_env_vars:
          PARAM_1: ${oc.env:PARAM_1}
          PARAM_2: ${oc.env:PARAM_2}
      libraries:
        - whl: ${oc.env:DATABRICKS_DBFS_DIR}/dist/${oc.env:WHEEL_NAME}
      spark_python_task:
        python_file: ${oc.env:DATABRICKS_DBFS_DIR}/databricks_stack_example/scripts/task1.py
        parameters:
          - "--param-1"
          - ${oc.env:PARAM_1}
          - "--param-2"
          - ${oc.env:PARAM_2}
      timeout_seconds: 7200
      max_retries: 1
  - id: multiple-task-job
    service: jobs
    properties:
      name: Example Multiple Task Job
      format: "MULTI_TASK"
      job_clusters:
        - job_cluster_key: multiple-task-job-cluster
          new_cluster:
            spark_version: 10.4.x-cpu-ml-scala2.12
            node_type_id: Standard_DS3_v2
            num_workers: 0
            spark_env_vars:
              PARAM_1: ${oc.env:PARAM_1}
              PARAM_2: ${oc.env:PARAM_2}
      tasks:
        - task_key: task-1
          description: Task 1
          job_cluster_key: multiple-task-job-cluster
          spark_python_task:
            python_file: ${oc.env:DATABRICKS_DBFS_DIR}/databricks_stack_example/scripts/task1.py
            parameters:
              - "--param-1"
              - "${oc.env:PARAM_1}"
              - "--param-2"
              - "${oc.env:PARAM_2}"
          libraries:
            - whl: ${oc.env:DATABRICKS_DBFS_DIR}/dist/${oc.env:WHEEL_NAME}
        - task_key: task-2
          description: Task 2
          job_cluster_key: multiple-task-job-cluster
          spark_python_task:
            python_file: ${oc.env:DATABRICKS_DBFS_DIR}/databricks_stack_example/scripts/task2.py
            parameters:
              - "--param-1"
              - "${oc.env:PARAM_1}"
              - "--param-2"
              - "${oc.env:PARAM_2}"
          libraries:
            - whl: ${oc.env:DATABRICKS_DBFS_DIR}/dist/${oc.env:WHEEL_NAME}
        - task_key: task-3
          description: Task 3
          depends_on:
            - task_key: task-1
            - task_key: task-2
          job_cluster_key: multiple-task-job-cluster
          spark_python_task:
            python_file: ${oc.env:DATABRICKS_DBFS_DIR}/databricks_stack_example/scripts/task3.py
            parameters:
              - "--param-1"
              - "${oc.env:PARAM_1}"
              - "--param-2"
              - "${oc.env:PARAM_2}"
          libraries:
            - whl: ${oc.env:DATABRICKS_DBFS_DIR}/dist/${oc.env:WHEEL_NAME}